{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec439cc8-d0b0-4213-9349-892d5faff3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER-Entity-Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b15472-46a3-4573-83a8-995192a0662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97021eb6-39a5-432c-8120-21786b67dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56912215-2fab-46a8-b103-632bd2d881f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the transformers NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")\n",
    "\n",
    "# Replace with your API key and endpoint\n",
    "API_KEY = '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'\n",
    "API_ENDPOINT = 'https://api-inference.huggingface.co/models/microsoft/Phi-3-mini-4k-instruct'\n",
    "\n",
    "def create_prompt(task_description, examples, input_sentence):\n",
    "    prompt = task_description + \"\\n\\n\"\n",
    "    for ex in examples:\n",
    "        prompt += f\"Input: {ex['input']}\\nOutput: {ex['output']}\\n\\n\"\n",
    "    prompt += f\"Input: {input_sentence}\\nOutput:\"\n",
    "    return prompt\n",
    "\n",
    "def extract_entities_with_categories(text):\n",
    "    entities = []\n",
    "    categories = {'name': [], 'location': [], 'date': [], 'organization': [], 'miscellaneous': []}\n",
    "    start_idx = text.find('@@')\n",
    "    while start_idx != -1:\n",
    "        end_idx = text.find('##', start_idx)\n",
    "        if end_idx != -1:\n",
    "            entity = text[start_idx + 2:end_idx]\n",
    "            entity_type = categorize_entity(entity)\n",
    "            categories[entity_type].append(entity)\n",
    "            entities.append((entity, entity_type))\n",
    "            start_idx = text.find('@@', end_idx)\n",
    "        else:\n",
    "            break\n",
    "    return categories\n",
    "\n",
    "def categorize_entity(entity):\n",
    "    # Use Hugging Face Transformers for entity categorization\n",
    "    ner_results = ner_pipeline(entity)\n",
    "    for result in ner_results:\n",
    "        label = result['entity_group']\n",
    "        if label == 'PER':\n",
    "            return 'name'\n",
    "        elif label in ['LOC', 'GPE']:\n",
    "            return 'location'\n",
    "        elif label == 'DATE':\n",
    "            return 'date'\n",
    "        elif label == 'ORG':\n",
    "            return 'organization'\n",
    "    # Fallback to heuristic categorization\n",
    "    names = [\"Aman\", \"Emily\", \"Barack Obama\", \"Bill Gates\", \"Elon Musk\", \"Taylor Swift\", \"Jeff Bezos\", \"Larry Page\", \"Sergey Brin\", \"Soham\"]\n",
    "    locations = [\"University of Sheffield\", \"Hawaii\", \"Cupertino\", \"Paris\", \"New York\", \"Great Wall of China\", \"Mount Everest\"]\n",
    "    date_pattern = re.compile(r'\\b\\d{2}/\\d{2}/\\d{2}\\b')\n",
    "    organization_keywords = [\"Inc.\", \"Ltd.\", \"Corp.\", \"LLC\", \"Company\", \"University\"]\n",
    "\n",
    "    if entity in names:\n",
    "        return 'name'\n",
    "    elif entity in locations:\n",
    "        return 'location'\n",
    "    elif date_pattern.match(entity):\n",
    "        return 'date'\n",
    "    elif any(keyword in entity for keyword in organization_keywords):\n",
    "        return 'organization'\n",
    "    else:\n",
    "        return 'miscellaneous'\n",
    "\n",
    "def train_model():\n",
    "    # Task description\n",
    "    task_description = (\n",
    "        \"I am an excellent linguist. The task is to label named entities in the given sentence.\"\n",
    "    )\n",
    "\n",
    "    # Few-shot examples\n",
    "    examples = [\n",
    "        {\"input\": \"Barack Obama was born in Hawaii on 04/08/61.\", \"output\": \"@@Barack Obama## was born in @@Hawaii## on @@04/08/61##.\"},\n",
    "        {\"input\": \"Microsoft was founded by Bill Gates on 04/04/75.\", \"output\": \"@@Microsoft## was founded by @@Bill Gates## on @@04/04/75##.\"},\n",
    "        {\"input\": \"Elon Musk is the CEO of SpaceX since 01/03/02.\", \"output\": \"@@Elon Musk## is the CEO of @@SpaceX## since @@01/03/02##.\"},\n",
    "        {\"input\": \"The Eiffel Tower was completed on 31/03/89.\", \"output\": \"The @@Eiffel Tower## was completed on @@31/03/89##.\"},\n",
    "        {\"input\": \"Apple Inc. was founded on 01/04/76.\", \"output\": \"@@Apple Inc.## was founded on @@01/04/76##.\"},\n",
    "        {\"input\": \"Amazon was started by Jeff Bezos on 05/07/94.\", \"output\": \"@@Amazon## was started by @@Jeff Bezos## on @@05/07/94##.\"},\n",
    "        {\"input\": \"Google was created by Larry Page and Sergey Brin on 04/09/98.\", \"output\": \"@@Google## was created by @@Larry Page## and @@Sergey Brin## on @@04/09/98##.\"},\n",
    "        {\"input\": \"Taylor Swift was born on 13/12/89.\", \"output\": \"@@Taylor Swift## was born on @@13/12/89##.\"},\n",
    "        {\"input\": \"The Great Wall of China was completed on 01/01/1644.\", \"output\": \"The @@Great Wall of China## was completed on @@01/01/1644##.\"},\n",
    "        {\"input\": \"Mount Everest was first climbed on 29/05/53.\", \"output\": \"@@Mount Everest## was first climbed on @@29/05/53##.\"}\n",
    "    ]\n",
    "\n",
    "    return task_description, examples\n",
    "\n",
    "def predict_ner(input_sentence, task_description, examples):\n",
    "    # Create the prompt\n",
    "    prompt = create_prompt(task_description, examples, input_sentence)\n",
    "\n",
    "    # Define the API request payload\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 250,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {API_KEY}'\n",
    "    }\n",
    "\n",
    "    # Send the request to the API\n",
    "    response = requests.post(API_ENDPOINT, headers=headers, data=json.dumps(payload))\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        response_data = response.json()\n",
    "        if isinstance(response_data, list) and len(response_data) > 0:\n",
    "            generated_output = response_data[0].get(\"generated_text\", \"\").strip()\n",
    "\n",
    "            # Extract the relevant output part for the specific input sentence\n",
    "            start_idx = generated_output.find(f\"Input: {input_sentence}\\nOutput:\")\n",
    "            if start_idx != -1:\n",
    "                relevant_part = generated_output[start_idx:].split(\"Output:\")[1].strip()\n",
    "                relevant_part = relevant_part.split(\"Input:\")[0].strip()\n",
    "            else:\n",
    "                relevant_part = \"\"\n",
    "\n",
    "            # Extract named entities with categories from the output\n",
    "            categories = extract_entities_with_categories(relevant_part)\n",
    "\n",
    "            # Display results\n",
    "            print(f\"Generated output: {relevant_part}\")\n",
    "            print(f\"Extracted entities with categories: {categories}\")\n",
    "        else:\n",
    "            print(\"The response did not contain any generated text.\")\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "\n",
    "# Train the model and get the task description and examples\n",
    "task_description, examples = train_model()\n",
    "\n",
    "# Use the trained model to predict named entities for a new complex input sentence\n",
    "predict_ner(\"On October 15, 2023, Dr. Susan Thompson, a renowned scientist from Harvard University, gave a groundbreaking presentation on climate change at the United Nations headquarters in New York City. The event was attended by representatives from NASA, Google, and several European countries including Germany and France. Additionally, prominent figures like Bill Gates and Elon Musk participated in the discussions, highlighting the need for global cooperation. Later in the evening, a gala dinner was held at the Ritz-Carlton Hotel, featuring performances by Beyonc√© and speeches by various dignitaries.\"\n",
    ", task_description, examples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc16b1-0d9a-4db1-8eff-3aa78fb35899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
